{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl-project-loremimpsum.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/tornermarton/deep_learning_project/blob/master/dl_project_loremimpsum.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "X6CGXvShrkRg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authorship identification using deep learning\n",
        "**Füleki Fábián,\tJani Balázs Gábor,\tTorner Márton**  \n",
        "*Project work for BME Deep Learning course (VITMAV45),  \n",
        "Team: LoremIpsum*\n",
        "\n",
        "\n",
        "**Milestone I**\n",
        "\n",
        "**Dataset:**  \n",
        "Our primary dataset is the Reuters_50_50 (C50), which is a subset of Reuters Corpus Volume I(RCVI). The RCV1 is archive of categorized newswire stories, made public for research purposes by Reuters, Ltd. The C50 collection consist of 50 texts for each of the 50 top author, for training and separately the same amount for testing purpose (5000 texts in total). This dataset has been previous used by previous studies of authorship recognition and can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3sKUlzl7FC9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "b5deb988-c3a2-48cd-e3e0-bb85e6346f78"
      },
      "cell_type": "code",
      "source": [
        "# Clean storage for new files\n",
        "!rm -r C50*\n",
        "\n",
        "# Download of the Reuter_50_50 (C50) dataset\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\"\n",
        "!unzip -q C50.zip\n",
        "\n",
        "# Download contains 2 directories split, merge them (we will do custom splitting)\n",
        "!mkdir C50\n",
        "!mv C50train/* C50/\n",
        "!rsync -a C50test/ C50/\n",
        "\n",
        "# Clean files we don't need\n",
        "!rm C50.zip\n",
        "!rm -r C50train\n",
        "!rm -r C50test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-14 21:33:48--  https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8194031 (7.8M) [application/zip]\n",
            "Saving to: ‘C50.zip’\n",
            "\n",
            "C50.zip             100%[===================>]   7.81M  8.15MB/s    in 1.0s    \n",
            "\n",
            "2018-10-14 21:33:50 (8.15 MB/s) - ‘C50.zip’ saved [8194031/8194031]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SecQn64b5N9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "193a1524-36ff-4b09-95a7-7da4ee5c7379"
      },
      "cell_type": "code",
      "source": [
        "# Download and install the largest language pack for SpaCy\n",
        "# It contains 1 000 000 word vectors (so only very rare words can't be processed)\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_lg\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_lg')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2NhHEv3P5QHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fc32e5bb-7c03-4df4-d0bc-3476819efda1"
      },
      "cell_type": "code",
      "source": [
        "# Get required resources\n",
        "import spacy\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np\n",
        "from nltk import tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "pd.set_option(\"max_columns\", None)\n",
        "nlp = spacy.load('en_core_web_lg', disable=['ner','parser'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QvDWMcmL7gJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_sentences_from_file(author, author_id, filename):\n",
        "  data = \"\"\n",
        "  sentences = []\n",
        "\n",
        "  # parse file\n",
        "  with open(\"C50/\"+author+\"/\"+filename, 'r') as file:\n",
        "      data=file.read()\n",
        "      \n",
        "  # split article into sentences\n",
        "  for sentence in tokenize.sent_tokenize(data):\n",
        "    # if a sentence is very short (happens e.g. after a quote 'he said.') we leave it out\n",
        "    if len(tokenize.word_tokenize(sentence)) > 7:\n",
        "      sentences.append([author_id, sentence])\n",
        "  \n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SujEA2XF_alz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# array which contains the authors' names\n",
        "authors = []\n",
        "\n",
        "def load_raw_sentences():\n",
        "  raw_sentences = []\n",
        "  authors = []\n",
        "  # read every file (articles) in the previously given root directory, the subdirectories are the authors' names\n",
        "  for root, dirs, files in os.walk(\"C50\"):\n",
        "    for dir in dirs:\n",
        "      authors.append(dir)\n",
        "      for file in os.listdir(\"C50/\"+dir):\n",
        "        raw_sentences.extend(read_sentences_from_file(dir, len(authors)-1, file))\n",
        "\n",
        "  return raw_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HJ6s1lWl248E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sentence parsing**\n",
        "\n",
        "We parse all the sentences with SpaCy in the followig way:\n",
        "\n",
        "1. Tokenize the sentence (split into words - in SpaCy the punctuation characters also count as words, but we remove them later, because they do not contain needed information)\n",
        "\n",
        "2. Get the vector form of each word, if it is not part of the largest collection (very rare words) we leave them out, because we can only use vectors for the inputs.\n",
        "\n",
        "3. Detect for each word which part of the sentence it is (part-of-speech tags - syntactic information)\n"
      ]
    },
    {
      "metadata": {
        "id": "NewjoP-KzrbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_raw_sentences(sentences, verbose=False):\n",
        "  # just for writing out fancy things\n",
        "  if verbose:\n",
        "    start_time = time.time()\n",
        "    \n",
        "  parsed_sentences = np.empty([len(sentences)], dtype=[('author', object, 1), ('original', object, 1), ('parsed', object, 1)])\n",
        "  \n",
        "  # parse every sentence (word splitting -> tokens, determine part-of-speech tags for every word)\n",
        "  for i in range(0, len(sentences)):\n",
        "    author = sentences[i][0]\n",
        "    raw_sentence = sentences[i][1]\n",
        "    parsed = np.array([], dtype=[('text', object, 1), ('vector', object, 1), ('pos_str', object, 1), ('pos_num', object, 1)])\n",
        "    \n",
        "    doc = nlp(raw_sentence)\n",
        "   \n",
        "    for token in doc:\n",
        "      # filter out stop words (not relevant/useful)\n",
        "      # 96 = punctuation char (->SpaCy documentation)\n",
        "      # if a word does not have vector form filter it out (very, very rare case)\n",
        "      if not token.is_stop and not token.pos == 96 and token.has_vector:\n",
        "        parsed = np.append(parsed, np.array((token.text, token.vector, token.pos_, token.pos), dtype=[('text', object, 1), ('vector', object, 1), ('pos_str', object, 1), ('pos_num', object, 1)]))\n",
        "    \n",
        "    parsed_sentences[i] = (author, raw_sentence, parsed)\n",
        "    \n",
        "    if verbose and (i+1)%1000 == 0:\n",
        "      print(str(i+1)+\" sentences parsed in \" +str(round(time.time() - start_time))+ \" seconds.\")\n",
        "  \n",
        "  if verbose:\n",
        "    print(str(len(sentences)) + \" sentences successfully parsed.\")\n",
        "    \n",
        "  \n",
        "  return parsed_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swUKZX2X9ZKq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_avg_sentence_len(sentences):\n",
        "  sum = 0\n",
        "  count = 0\n",
        "  for sentence in sentences['parsed']:\n",
        "    sum += len(sentence)\n",
        "    count += 1\n",
        "  \n",
        "  return sum/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6suItkDJ29xG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Equalization of the sentences in the dataset**\n",
        "\n",
        "We plan to use sentence based identification so out system needs sentences which have equal lengths (word count), but obviously the articles are not written in this way, so we have to make the equalization. We calculate the average word count of the sentences in the dataset and then we transform all of them to contain the same number of words (we round up the average to keep more sentences in the full form).\n",
        "\n",
        "Too short sentences are extended with wildcard (magic) words which will be filtered out in a way in the learning process.\n",
        "\n",
        "Too long sentences are simply cut to shape."
      ]
    },
    {
      "metadata": {
        "id": "MN1hSThT8s3d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def equalize_sentence_len(sentences):\n",
        "  # calculate the average sentence length and round it up (we try to keep most of the sentences)\n",
        "  avg = math.ceil(count_avg_sentence_len(sentences))\n",
        "  \n",
        "  # process \n",
        "  for i in range(0, len(sentences['parsed'])):\n",
        "    sentence = sentences['parsed'][i]\n",
        "    \n",
        "    # 'magic words' : text = Xxxxxx ; vector=nullvector ; pos_tag='' ; pos_tag number form : 0\n",
        "    # insert magic word into random positions for every sentence, which is too short (shorter, than average)\n",
        "    while len(sentence) < avg:\n",
        "      idx = np.random.randint(len(sentence))\n",
        "      sentence = np.insert(sentence, idx, np.array((\"Xxxxxx\", np.zeros(300), \"\", 0), dtype=[('text', object, 1), ('vector', object, 1), ('pos_str', object, 1), ('pos_num', object, 1)]), axis=0)\n",
        "    \n",
        "    # if sentence is too long cut it\n",
        "    if len(sentence) > avg:\n",
        "      sentence = sentence[0:avg]\n",
        "      \n",
        "    sentences['parsed'][i] = sentence\n",
        "      \n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f_gAUg9hFycB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "38465696-c400-42e3-e108-9b3ca48d18b3"
      },
      "cell_type": "code",
      "source": [
        "raw_sentences = load_raw_sentences()\n",
        "print(\"Total number of the loaded sentences: \" + str(len(raw_sentences)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of the loaded sentences: 105337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eFlD8nkey7bA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "ea9da25c-58ca-43bd-e344-ad063faf4b32"
      },
      "cell_type": "code",
      "source": [
        "#load only the first 10000 sentences for demonstration (parsing 100000+ sentences would be 10+ minutes)\n",
        "dataset = parse_raw_sentences(raw_sentences[0:10000], True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 sentences parsed in 6 seconds.\n",
            "2000 sentences parsed in 12 seconds.\n",
            "3000 sentences parsed in 18 seconds.\n",
            "4000 sentences parsed in 25 seconds.\n",
            "5000 sentences parsed in 31 seconds.\n",
            "6000 sentences parsed in 37 seconds.\n",
            "7000 sentences parsed in 43 seconds.\n",
            "8000 sentences parsed in 49 seconds.\n",
            "9000 sentences parsed in 55 seconds.\n",
            "10000 sentences parsed in 62 seconds.\n",
            "10000 sentences successfully parsed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x8sAM7X8_fPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bfb7e351-ca4d-4e97-a000-86f645403613"
      },
      "cell_type": "code",
      "source": [
        "avg_before = count_avg_sentence_len(dataset)\n",
        "print(\"The average sentence length before the equalization: \" + str(avg_before))\n",
        "\n",
        "# equalize the length of the sentences (we need the number of words to be equal)\n",
        "dataset = equalize_sentence_len(dataset)\n",
        "\n",
        "avg_after = count_avg_sentence_len(dataset)\n",
        "print(\"And after: \" + str(avg_after))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average sentence length before the equalization: 23.0398\n",
            "And after: 24.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x5IMsfD36Gpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d4aaddb1-496f-46ae-ca8b-3ac4ec41acee"
      },
      "cell_type": "code",
      "source": [
        "print(\"One sample sentence:\\n\")\n",
        "print(\"Author: \" + str(dataset[\"author\"][0]) )\n",
        "print(\"Sentence: \" + dataset[\"original\"][0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One sample sentence:\n",
            "\n",
            "Author: 0\n",
            "Sentence: Bulk cocoa shipments from West Africa will more than double to 325,000 tonnes in 1996/97, solidifying a cost-cutting trend sparked by recent trial shipments, exporters and shippers said.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AenGC9te0ub4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "249e4d6c-693d-4861-a718-92499c57a029"
      },
      "cell_type": "code",
      "source": [
        "print(\"The parsed form of the above sentence:\")\n",
        "df = pd.DataFrame(data=dataset[\"parsed\"][0])\n",
        "df.T"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parsed form of the above sentence:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>Bulk</td>\n",
              "      <td>cocoa</td>\n",
              "      <td>shipments</td>\n",
              "      <td>from</td>\n",
              "      <td>West</td>\n",
              "      <td>Africa</td>\n",
              "      <td>will</td>\n",
              "      <td>more</td>\n",
              "      <td>than</td>\n",
              "      <td>double</td>\n",
              "      <td>to</td>\n",
              "      <td>325,000</td>\n",
              "      <td>tonnes</td>\n",
              "      <td>in</td>\n",
              "      <td>1996/97</td>\n",
              "      <td>solidifying</td>\n",
              "      <td>a</td>\n",
              "      <td>cost</td>\n",
              "      <td>cutting</td>\n",
              "      <td>trend</td>\n",
              "      <td>sparked</td>\n",
              "      <td>by</td>\n",
              "      <td>recent</td>\n",
              "      <td>trial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vector</th>\n",
              "      <td>[-0.28515, 0.26314, -0.16877, 0.36536, -0.1552...</td>\n",
              "      <td>[0.1582, -0.072933, 0.1475, -0.1833, -0.45731,...</td>\n",
              "      <td>[-0.27237, 0.37706, 0.44835, 0.2931, 0.22239, ...</td>\n",
              "      <td>[0.01332, -0.051085, -0.13207, 0.40386, 0.2113...</td>\n",
              "      <td>[0.22047, 0.023535, 0.61011, -0.17253, 1.132, ...</td>\n",
              "      <td>[-0.44178, 0.14558, 0.47388, -0.41953, 0.52292...</td>\n",
              "      <td>[0.027165, 0.29879, -0.019263, -0.0043049, -0....</td>\n",
              "      <td>[-0.39717, 0.30269, -0.18428, -0.065407, 0.196...</td>\n",
              "      <td>[-0.39611, 0.18991, -0.020033, -0.39995, 0.190...</td>\n",
              "      <td>[-0.10505, 0.15456, -0.25162, 0.017396, 0.1377...</td>\n",
              "      <td>[0.31924, 0.06316, -0.27858, 0.2612, 0.079248,...</td>\n",
              "      <td>[-0.0030893, -0.106, 0.29317, -0.22709, -0.201...</td>\n",
              "      <td>[-0.16118, 0.25736, -0.0994, 0.3978, -0.11761,...</td>\n",
              "      <td>[0.089187, 0.25792, 0.26282, -0.029365, 0.4718...</td>\n",
              "      <td>[-0.045365, -0.43793, 0.39185, 0.22654, 0.2276...</td>\n",
              "      <td>[0.61367, -0.092586, 0.18408, 0.15051, 0.48288...</td>\n",
              "      <td>[0.043798, 0.024779, -0.20937, 0.49745, 0.3601...</td>\n",
              "      <td>[-0.89423, 0.39636, 0.64359, -0.19608, -0.0955...</td>\n",
              "      <td>[-0.31639, 0.61819, 0.18432, -0.51989, 0.06971...</td>\n",
              "      <td>[-0.043012, -0.027765, 0.27702, -0.032487, 0.5...</td>\n",
              "      <td>[-0.1655, 0.14283, 0.50184, 0.54028, 0.089523,...</td>\n",
              "      <td>[-0.15552, -0.33723, -0.097191, -0.21617, -0.3...</td>\n",
              "      <td>[-0.33847, 0.058326, 0.098077, 0.20065, 0.1233...</td>\n",
              "      <td>[-0.20203, 0.12291, -0.045195, -0.0055856, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pos_str</th>\n",
              "      <td>ADJ</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>ADP</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>VERB</td>\n",
              "      <td>ADV</td>\n",
              "      <td>ADP</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NUM</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NUM</td>\n",
              "      <td>VERB</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>VERB</td>\n",
              "      <td>ADP</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pos_num</th>\n",
              "      <td>83</td>\n",
              "      <td>91</td>\n",
              "      <td>91</td>\n",
              "      <td>84</td>\n",
              "      <td>95</td>\n",
              "      <td>95</td>\n",
              "      <td>99</td>\n",
              "      <td>85</td>\n",
              "      <td>84</td>\n",
              "      <td>83</td>\n",
              "      <td>84</td>\n",
              "      <td>92</td>\n",
              "      <td>91</td>\n",
              "      <td>84</td>\n",
              "      <td>92</td>\n",
              "      <td>99</td>\n",
              "      <td>89</td>\n",
              "      <td>91</td>\n",
              "      <td>99</td>\n",
              "      <td>91</td>\n",
              "      <td>99</td>\n",
              "      <td>84</td>\n",
              "      <td>83</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        0   \\\n",
              "text                                                  Bulk   \n",
              "vector   [-0.28515, 0.26314, -0.16877, 0.36536, -0.1552...   \n",
              "pos_str                                                ADJ   \n",
              "pos_num                                                 83   \n",
              "\n",
              "                                                        1   \\\n",
              "text                                                 cocoa   \n",
              "vector   [0.1582, -0.072933, 0.1475, -0.1833, -0.45731,...   \n",
              "pos_str                                               NOUN   \n",
              "pos_num                                                 91   \n",
              "\n",
              "                                                        2   \\\n",
              "text                                             shipments   \n",
              "vector   [-0.27237, 0.37706, 0.44835, 0.2931, 0.22239, ...   \n",
              "pos_str                                               NOUN   \n",
              "pos_num                                                 91   \n",
              "\n",
              "                                                        3   \\\n",
              "text                                                  from   \n",
              "vector   [0.01332, -0.051085, -0.13207, 0.40386, 0.2113...   \n",
              "pos_str                                                ADP   \n",
              "pos_num                                                 84   \n",
              "\n",
              "                                                        4   \\\n",
              "text                                                  West   \n",
              "vector   [0.22047, 0.023535, 0.61011, -0.17253, 1.132, ...   \n",
              "pos_str                                              PROPN   \n",
              "pos_num                                                 95   \n",
              "\n",
              "                                                        5   \\\n",
              "text                                                Africa   \n",
              "vector   [-0.44178, 0.14558, 0.47388, -0.41953, 0.52292...   \n",
              "pos_str                                              PROPN   \n",
              "pos_num                                                 95   \n",
              "\n",
              "                                                        6   \\\n",
              "text                                                  will   \n",
              "vector   [0.027165, 0.29879, -0.019263, -0.0043049, -0....   \n",
              "pos_str                                               VERB   \n",
              "pos_num                                                 99   \n",
              "\n",
              "                                                        7   \\\n",
              "text                                                  more   \n",
              "vector   [-0.39717, 0.30269, -0.18428, -0.065407, 0.196...   \n",
              "pos_str                                                ADV   \n",
              "pos_num                                                 85   \n",
              "\n",
              "                                                        8   \\\n",
              "text                                                  than   \n",
              "vector   [-0.39611, 0.18991, -0.020033, -0.39995, 0.190...   \n",
              "pos_str                                                ADP   \n",
              "pos_num                                                 84   \n",
              "\n",
              "                                                        9   \\\n",
              "text                                                double   \n",
              "vector   [-0.10505, 0.15456, -0.25162, 0.017396, 0.1377...   \n",
              "pos_str                                                ADJ   \n",
              "pos_num                                                 83   \n",
              "\n",
              "                                                        10  \\\n",
              "text                                                    to   \n",
              "vector   [0.31924, 0.06316, -0.27858, 0.2612, 0.079248,...   \n",
              "pos_str                                                ADP   \n",
              "pos_num                                                 84   \n",
              "\n",
              "                                                        11  \\\n",
              "text                                               325,000   \n",
              "vector   [-0.0030893, -0.106, 0.29317, -0.22709, -0.201...   \n",
              "pos_str                                                NUM   \n",
              "pos_num                                                 92   \n",
              "\n",
              "                                                        12  \\\n",
              "text                                                tonnes   \n",
              "vector   [-0.16118, 0.25736, -0.0994, 0.3978, -0.11761,...   \n",
              "pos_str                                               NOUN   \n",
              "pos_num                                                 91   \n",
              "\n",
              "                                                        13  \\\n",
              "text                                                    in   \n",
              "vector   [0.089187, 0.25792, 0.26282, -0.029365, 0.4718...   \n",
              "pos_str                                                ADP   \n",
              "pos_num                                                 84   \n",
              "\n",
              "                                                        14  \\\n",
              "text                                               1996/97   \n",
              "vector   [-0.045365, -0.43793, 0.39185, 0.22654, 0.2276...   \n",
              "pos_str                                                NUM   \n",
              "pos_num                                                 92   \n",
              "\n",
              "                                                        15  \\\n",
              "text                                           solidifying   \n",
              "vector   [0.61367, -0.092586, 0.18408, 0.15051, 0.48288...   \n",
              "pos_str                                               VERB   \n",
              "pos_num                                                 99   \n",
              "\n",
              "                                                        16  \\\n",
              "text                                                     a   \n",
              "vector   [0.043798, 0.024779, -0.20937, 0.49745, 0.3601...   \n",
              "pos_str                                                DET   \n",
              "pos_num                                                 89   \n",
              "\n",
              "                                                        17  \\\n",
              "text                                                  cost   \n",
              "vector   [-0.89423, 0.39636, 0.64359, -0.19608, -0.0955...   \n",
              "pos_str                                               NOUN   \n",
              "pos_num                                                 91   \n",
              "\n",
              "                                                        18  \\\n",
              "text                                               cutting   \n",
              "vector   [-0.31639, 0.61819, 0.18432, -0.51989, 0.06971...   \n",
              "pos_str                                               VERB   \n",
              "pos_num                                                 99   \n",
              "\n",
              "                                                        19  \\\n",
              "text                                                 trend   \n",
              "vector   [-0.043012, -0.027765, 0.27702, -0.032487, 0.5...   \n",
              "pos_str                                               NOUN   \n",
              "pos_num                                                 91   \n",
              "\n",
              "                                                        20  \\\n",
              "text                                               sparked   \n",
              "vector   [-0.1655, 0.14283, 0.50184, 0.54028, 0.089523,...   \n",
              "pos_str                                               VERB   \n",
              "pos_num                                                 99   \n",
              "\n",
              "                                                        21  \\\n",
              "text                                                    by   \n",
              "vector   [-0.15552, -0.33723, -0.097191, -0.21617, -0.3...   \n",
              "pos_str                                                ADP   \n",
              "pos_num                                                 84   \n",
              "\n",
              "                                                        22  \\\n",
              "text                                                recent   \n",
              "vector   [-0.33847, 0.058326, 0.098077, 0.20065, 0.1233...   \n",
              "pos_str                                                ADJ   \n",
              "pos_num                                                 83   \n",
              "\n",
              "                                                        23  \n",
              "text                                                 trial  \n",
              "vector   [-0.20203, 0.12291, -0.045195, -0.0055856, -0....  \n",
              "pos_str                                               NOUN  \n",
              "pos_num                                                 91  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "9A1clC6ukxdH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reworked_dataset = np.empty([len(dataset)], dtype=[('input', object, 1), ('output', object, 1)])\n",
        "for j in range(0, len(dataset)):\n",
        "    output_ = dataset[j]['author']\n",
        "    \n",
        "   \n",
        "    for i in range(len(dataset[j]['parsed']['vector'])):\n",
        "        input_ = (np.append(dataset[j]['parsed']['vector'][i], dataset[j][\"parsed\"][\"pos_num\"][i]/100))\n",
        "        \n",
        "    reworked_dataset[j] = (input_, output_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JEWtpBuhKc-a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.shuffle(reworked_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hK_-uEO42DOr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_samples = len(reworked_dataset)\n",
        "valid_split = 0.2\n",
        "test_split = 0.1\n",
        "\n",
        "# train-valid-test split\n",
        "X_train = reworked_dataset['input'][0:int(nb_samples*(1-valid_split-test_split))]\n",
        "X_valid = reworked_dataset['input'][int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
        "X_test  = reworked_dataset['input'][int(nb_samples*(1-test_split)):]\n",
        "\n",
        "Y_train = reworked_dataset['output'][0:int(nb_samples*(1-valid_split-test_split))]\n",
        "Y_valid = reworked_dataset['output'][int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
        "Y_test  = reworked_dataset['output'][int(nb_samples*(1-test_split)):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dvyOE1FzUsHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "63aee228-c232-4437-b798-6e4b8cb3c4c1"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "Y_train = np_utils.to_categorical(Y_train, 50) # one hot encoding\n",
        "Y_valid = np_utils.to_categorical(Y_valid, 50)\n",
        "Y_test = np_utils.to_categorical(Y_test, 50)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fRle2IS9U2JH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}