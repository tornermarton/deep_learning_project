{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship identification using deep learning\n",
    "**Füleki Fábián,\tJani Balázs Gábor,\tTorner Márton**  \n",
    "*Project work for BME Deep Learning course (VITMAV45),  \n",
    "Team: LoremIpsum*\n",
    "\n",
    "**Dataset:**  \n",
    "Our primary dataset is the Reuters_50_50 (C50), which is a subset of Reuters Corpus Volume I(RCVI). The RCV1 is archive of categorized newswire stories, made public for research purposes by Reuters, Ltd. The C50 collection consist of 50 texts for each of the 50 top author, for training and separately the same amount for testing purpose (5000 texts in total). This dataset has been previous used by previous studies of authorship recognition and can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Get required resources\n",
    "import spacy\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk import tokenize\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "np.random.seed(1234)\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-09 10:00:12--  https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8194031 (7.8M) [application/zip]\n",
      "Saving to: ‘C50.zip’\n",
      "\n",
      "C50.zip             100%[===================>]   7.81M  5.00MB/s    in 1.6s    \n",
      "\n",
      "2018-12-09 10:00:14 (5.00 MB/s) - ‘C50.zip’ saved [8194031/8194031]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean storage for new files\n",
    "!rm -r C50*\n",
    "\n",
    "# Download of the Reuter_50_50 (C50) dataset\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\"\n",
    "!unzip -q C50.zip\n",
    "\n",
    "# Download contains 2 directories split, merge them (we will do custom splitting)\n",
    "!mkdir C50\n",
    "!mv C50train/* C50/\n",
    "!rsync -a C50test/ C50/\n",
    "\n",
    "# Clean files we don't need\n",
    "!rm C50.zip\n",
    "!rm -r C50train\n",
    "!rm -r C50test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 852.3MB 129.0MB/s ta 0:00:01��████████████████        | 640.2MB 123.2MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "  Running setup.py install for en-core-web-lg ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-lg-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/ffabi1997/.local/lib/python2.7/site-packages/en_core_web_lg -->\n",
      "    /home/ffabi1997/.local/lib/python2.7/site-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and install the largest language pack for SpaCy\n",
    "# It contains 1 000 000 word vectors (so only very rare words can't be processed)\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ffabi1997/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "pd.set_option(\"max_columns\", None)\n",
    "# for faster parsing we disable the components we don't use\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors must be represented as numbers so we create\n",
    "# a list and the indexes are the repesentations - easy translation\n",
    "\n",
    "# array which contains the authors' names\n",
    "authors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(root_dir, author, author_id, filename):\n",
    "  sentences = []\n",
    "\n",
    "  # read the content\n",
    "  with open(root_dir+\"/\"+author+\"/\"+filename, 'r') as file:\n",
    "      data=file.read()\n",
    "      \n",
    "  # split article into sentences\n",
    "  for sentence in tokenize.sent_tokenize(data):\n",
    "    sentences.append([author_id, sentence])\n",
    "  \n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_sentences(root_dir, articles_per_author):\n",
    "  raw_sentences = []\n",
    "  n_sentences = 0\n",
    "  authors = []\n",
    "  \n",
    "  # read every file (articles) in the previously given root directory - the subdirectories are the authors' names\n",
    "  for root, dirs, files in os.walk(root_dir):\n",
    "    for dir in dirs:\n",
    "      authors.append(str(dir))\n",
    "      author_sentences = []\n",
    "      \n",
    "      for i, file in enumerate(os.listdir(root_dir+\"/\"+dir)):\n",
    "        sentences = read_sentences_from_file(root_dir, dir, len(authors)-1, file)\n",
    "        \n",
    "        n_sentences += len(sentences)\n",
    "        author_sentences.append(sentences)\n",
    "        \n",
    "        if i == articles_per_author-1: \n",
    "          break\n",
    "          \n",
    "      raw_sentences.append(author_sentences)\n",
    "\n",
    "  return raw_sentences, n_sentences, authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence parsing**\n",
    "\n",
    "We parse all the sentences with SpaCy in the followig way:\n",
    "\n",
    "1. Tokenize the sentence (split into words - in SpaCy the punctuation characters also count as words, but we remove them later, because they do not contain relevant information also stop words are removed for the same reason)\n",
    "\n",
    "2. Get the vector form of each word, if it is not part of the largest collection (very rare words) we leave them out, because we can only use vectors for the inputs.\n",
    "\n",
    "3. Detect for each word which part of the sentence it is (part-of-speech tags - syntactic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample sentence:\n",
      "\n",
      "Author: AaronPressman\n",
      "Sentence: The Internet may be overflowing with new technology but crime in cyberspace is still of the old-fashioned variety.\n",
      "\n",
      "\n",
      "Parsed form:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>The</td>\n",
       "      <td>Internet</td>\n",
       "      <td>may</td>\n",
       "      <td>be</td>\n",
       "      <td>overflowing</td>\n",
       "      <td>with</td>\n",
       "      <td>new</td>\n",
       "      <td>technology</td>\n",
       "      <td>but</td>\n",
       "      <td>crime</td>\n",
       "      <td>in</td>\n",
       "      <td>cyberspace</td>\n",
       "      <td>is</td>\n",
       "      <td>still</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>old</td>\n",
       "      <td>fashioned</td>\n",
       "      <td>variety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector</th>\n",
       "      <td>[0.27204, -0.06203, -0.1884, 0.023225, -0.0181...</td>\n",
       "      <td>[-0.50955, 0.088231, -0.32273, -0.40398, 0.003...</td>\n",
       "      <td>[-0.042501, 0.090773, -0.11918, 0.12372, -0.19...</td>\n",
       "      <td>[-0.059177, 0.10653, -0.21613, -0.086178, 0.00...</td>\n",
       "      <td>[0.074908, -0.036973, 0.082992, -0.31622, 0.22...</td>\n",
       "      <td>[-0.099534, 0.028202, -0.23189, 0.094477, 0.12...</td>\n",
       "      <td>[0.34046, 0.13752, -0.20643, -0.4555, 0.19251,...</td>\n",
       "      <td>[-0.32298, 0.38883, 0.4586, -0.5227, -0.064451...</td>\n",
       "      <td>[-0.01689, 0.17402, -0.30247, -0.30063, 0.2141...</td>\n",
       "      <td>[-0.43159, 0.22378, -0.03975, -0.5106, 0.22443...</td>\n",
       "      <td>[0.089187, 0.25792, 0.26282, -0.029365, 0.4718...</td>\n",
       "      <td>[0.031095, -0.24727, 0.076433, -0.018738, 0.34...</td>\n",
       "      <td>[-0.084961, 0.502, 0.0023823, -0.16755, 0.3072...</td>\n",
       "      <td>[0.11259, 0.1539, -0.14328, -0.18177, 0.12315,...</td>\n",
       "      <td>[0.060216, 0.21799, -0.04249, -0.38618, -0.153...</td>\n",
       "      <td>[0.27204, -0.06203, -0.1884, 0.023225, -0.0181...</td>\n",
       "      <td>[0.26105, -0.043804, -0.3964, 0.022796, -0.040...</td>\n",
       "      <td>[-0.082367, -0.22914, -0.191, 0.0044482, -0.29...</td>\n",
       "      <td>[-0.22488, 0.020037, 0.08535, -0.27456, 0.5060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_str</th>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_num</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0   \\\n",
       "text                                                   The   \n",
       "vector   [0.27204, -0.06203, -0.1884, 0.023225, -0.0181...   \n",
       "pos_str                                                DET   \n",
       "pos_num                                               0.89   \n",
       "\n",
       "                                                        1   \\\n",
       "text                                              Internet   \n",
       "vector   [-0.50955, 0.088231, -0.32273, -0.40398, 0.003...   \n",
       "pos_str                                               NOUN   \n",
       "pos_num                                               0.91   \n",
       "\n",
       "                                                        2   \\\n",
       "text                                                   may   \n",
       "vector   [-0.042501, 0.090773, -0.11918, 0.12372, -0.19...   \n",
       "pos_str                                               VERB   \n",
       "pos_num                                               0.99   \n",
       "\n",
       "                                                        3   \\\n",
       "text                                                    be   \n",
       "vector   [-0.059177, 0.10653, -0.21613, -0.086178, 0.00...   \n",
       "pos_str                                               VERB   \n",
       "pos_num                                               0.99   \n",
       "\n",
       "                                                        4   \\\n",
       "text                                           overflowing   \n",
       "vector   [0.074908, -0.036973, 0.082992, -0.31622, 0.22...   \n",
       "pos_str                                               VERB   \n",
       "pos_num                                               0.99   \n",
       "\n",
       "                                                        5   \\\n",
       "text                                                  with   \n",
       "vector   [-0.099534, 0.028202, -0.23189, 0.094477, 0.12...   \n",
       "pos_str                                                ADP   \n",
       "pos_num                                               0.84   \n",
       "\n",
       "                                                        6   \\\n",
       "text                                                   new   \n",
       "vector   [0.34046, 0.13752, -0.20643, -0.4555, 0.19251,...   \n",
       "pos_str                                                ADJ   \n",
       "pos_num                                               0.83   \n",
       "\n",
       "                                                        7   \\\n",
       "text                                            technology   \n",
       "vector   [-0.32298, 0.38883, 0.4586, -0.5227, -0.064451...   \n",
       "pos_str                                               NOUN   \n",
       "pos_num                                               0.91   \n",
       "\n",
       "                                                        8   \\\n",
       "text                                                   but   \n",
       "vector   [-0.01689, 0.17402, -0.30247, -0.30063, 0.2141...   \n",
       "pos_str                                              CCONJ   \n",
       "pos_num                                               0.88   \n",
       "\n",
       "                                                        9   \\\n",
       "text                                                 crime   \n",
       "vector   [-0.43159, 0.22378, -0.03975, -0.5106, 0.22443...   \n",
       "pos_str                                               NOUN   \n",
       "pos_num                                               0.91   \n",
       "\n",
       "                                                        10  \\\n",
       "text                                                    in   \n",
       "vector   [0.089187, 0.25792, 0.26282, -0.029365, 0.4718...   \n",
       "pos_str                                                ADP   \n",
       "pos_num                                               0.84   \n",
       "\n",
       "                                                        11  \\\n",
       "text                                            cyberspace   \n",
       "vector   [0.031095, -0.24727, 0.076433, -0.018738, 0.34...   \n",
       "pos_str                                               NOUN   \n",
       "pos_num                                               0.91   \n",
       "\n",
       "                                                        12  \\\n",
       "text                                                    is   \n",
       "vector   [-0.084961, 0.502, 0.0023823, -0.16755, 0.3072...   \n",
       "pos_str                                               VERB   \n",
       "pos_num                                               0.99   \n",
       "\n",
       "                                                        13  \\\n",
       "text                                                 still   \n",
       "vector   [0.11259, 0.1539, -0.14328, -0.18177, 0.12315,...   \n",
       "pos_str                                                ADV   \n",
       "pos_num                                               0.85   \n",
       "\n",
       "                                                        14  \\\n",
       "text                                                    of   \n",
       "vector   [0.060216, 0.21799, -0.04249, -0.38618, -0.153...   \n",
       "pos_str                                                ADP   \n",
       "pos_num                                               0.84   \n",
       "\n",
       "                                                        15  \\\n",
       "text                                                   the   \n",
       "vector   [0.27204, -0.06203, -0.1884, 0.023225, -0.0181...   \n",
       "pos_str                                                DET   \n",
       "pos_num                                               0.89   \n",
       "\n",
       "                                                        16  \\\n",
       "text                                                   old   \n",
       "vector   [0.26105, -0.043804, -0.3964, 0.022796, -0.040...   \n",
       "pos_str                                                ADJ   \n",
       "pos_num                                               0.83   \n",
       "\n",
       "                                                        17  \\\n",
       "text                                             fashioned   \n",
       "vector   [-0.082367, -0.22914, -0.191, 0.0044482, -0.29...   \n",
       "pos_str                                                ADJ   \n",
       "pos_num                                               0.83   \n",
       "\n",
       "                                                        18  \n",
       "text                                               variety  \n",
       "vector   [-0.22488, 0.020037, 0.08535, -0.27456, 0.5060...  \n",
       "pos_str                                               NOUN  \n",
       "pos_num                                               0.91  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_article = read_sentences_from_file(\"C50\", \"AaronPressman\", 0, \"106247newsML.txt\")\n",
    "\n",
    "sentence = sample_article[0][1]\n",
    "        \n",
    "parsed = np.array([], dtype=[('text', object, 1), ('vector', object, 1), ('pos_str', object, 1), ('pos_num', object, 1)])\n",
    "    \n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "  # filter out stop words (not relevant/useful)\n",
    "  # 96 = punctuation char (->SpaCy documentation)\n",
    "  # if a word does not have vector form filter it out (very, very rare case)\n",
    "  if not token.is_stop and not token.pos == 96 and token.has_vector:\n",
    "    parsed = np.append(parsed, np.array((token.text, token.vector, token.pos_, token.pos/100), dtype=[('text', object, 1), ('vector', object, 1), ('pos_str', object, 1), ('pos_num', object, 1)]))\n",
    "\n",
    "print(\"One sample sentence:\\n\")\n",
    "print(\"Author: \" + \"AaronPressman\")\n",
    "print(\"Sentence: \" + sentence)\n",
    "print(\"\\n\")\n",
    "print(\"Parsed form:\\n\")\n",
    "df = pd.DataFrame(data=parsed)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equalization of the sentences in the dataset**\n",
    "\n",
    "We plan to use sentence based identification so out system needs sentences which have equal lengths (word count), but obviously the articles are not written in this way, so we have to make the equalization.\n",
    "\n",
    "Too short sentences (sentence chunks) are extended with wildcard (magic) words which will be filtered out in a way in the learning process.\n",
    "\n",
    "Too long sentences (sentence chunks) are simply cut to shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(root_dir, chunk_size=1, articles_per_author=100, verbose=True):\n",
    "  \n",
    "  # just for writing out fancy things\n",
    "  if verbose:\n",
    "    start_time = time.time()\n",
    "    s = \"\"\n",
    "  \n",
    "  # the length of a chunk - \n",
    "  # if the chunk of sentences is longer we cut the end, if shorter we leave 0s at the end (magic words)\n",
    "  n_words_per_chunk = chunk_size*25\n",
    "  # the length of the word vector\n",
    "  word_repr_vector_size = 301\n",
    "  \n",
    "  # numpy array shape\n",
    "  sentence_shape = (n_words_per_chunk , word_repr_vector_size)\n",
    "  \n",
    "  raw_sentences, n_sentences, authors = load_raw_sentences(root_dir, articles_per_author)\n",
    "  \n",
    "  n_articles = len(raw_sentences)*len(raw_sentences[0])\n",
    "  \n",
    "  if verbose:\n",
    "    print(\"Sentences loaded from files. Start parsing.\")\n",
    "  \n",
    "  # the final dataset will be shorter, because the first and last sentences don't have enough surrounding sentences)\n",
    "  dataset = np.zeros([n_sentences-(chunk_size-1)*n_articles], dtype=[('input', np.float32, sentence_shape), ('output', np.float32, 1)])\n",
    "  \n",
    "  # index dataset\n",
    "  index = 0\n",
    "  \n",
    "  for i, author in enumerate(raw_sentences):\n",
    "    for j, article in enumerate(author):\n",
    "      \n",
    "      article_parsed = np.empty([len(article)], dtype=object)\n",
    "      # author is the same for every sentence\n",
    "      author_id = article[0][0]\n",
    "      \n",
    "      # parse sentences in article\n",
    "      # we do this in advance because this is the longest process and the overlapping creates huge redundance\n",
    "      for k in range(0, len(article)):\n",
    "        sentence = raw_sentences[i][j][k][1]\n",
    "        \n",
    "        parsed = np.empty((0, 301), np.float32)\n",
    "  \n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        for token in doc:\n",
    "          # filter out stop words (not relevant/useful)\n",
    "          # 96 = punctuation char (->SpaCy documentation)\n",
    "          # if a word does not have vector form filter it out (very, very rare case)\n",
    "          if not token.is_stop and not token.pos == 96 and token.has_vector:\n",
    "            parsed = np.append(parsed, np.array([np.append(token.vector, float(token.pos)/100)]), axis=0)\n",
    "            \n",
    "        article_parsed[k] = parsed\n",
    "        \n",
    "      # for \"every sentence\" (the result is shorter, because the first and last sentences don't have enough neighbours)\n",
    "      for k in range(0, len(article_parsed)-chunk_size+1):\n",
    "        cursor = 0\n",
    "        dataset[index][\"output\"] = author_id\n",
    "    \n",
    "        # itarate for chunk_size from the actual sentence (eg. chunk_size = 3: 1st 2nd and 3rd sentence ... n-2, n-1, n sentence)\n",
    "        for l in range(k, k+chunk_size):\n",
    "          for m in range(0, len(article_parsed[l])):\n",
    "            dataset[index][\"input\"][cursor] = article_parsed[l][m]\n",
    "            cursor += 1\n",
    "            if(cursor == n_words_per_chunk): break;\n",
    "          if(cursor == n_words_per_chunk): break;\n",
    "            \n",
    "        index += 1\n",
    "      \n",
    "      # just for writing out fancy things\n",
    "      if verbose:\n",
    "        s =  str(i*articles_per_author+j+1)+ \"/\" + str(n_articles) +\" articles parsed in \" +str(round(time.time() - start_time))+ \" seconds.\"\n",
    "\n",
    "        cnt = int((i*articles_per_author+j+1)/n_articles*50)\n",
    "\n",
    "        sys.stdout.write('\\r'+ \"Processing. [\" + \"=\"*cnt + \">\" + \" \"*(50-cnt) + \"] \" + s )\n",
    "       \n",
    "  return dataset, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences loaded from files. Start parsing.\n",
      "Processing. [==================================================>] 5000/5000 articles parsed in 483 seconds."
     ]
    }
   ],
   "source": [
    "# ~10 min for chunk_size=3 with full database\n",
    "dataset, authors = create_dataset(\"C50\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98473, 75, 301)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/authors_chunk3.serialized\", authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/dataset_chunk3.serialized\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
