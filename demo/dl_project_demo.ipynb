{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ1TffwT4zq1"
   },
   "source": [
    "# LoremIpsum - Authorship Identification on Reuters_50_50\n",
    "\n",
    "**Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_3IQTrJd4mJn",
    "outputId": "ac548240-f304-43fe-e747-036fed14325c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_columns\", None)\n",
    "\n",
    "from nltk import tokenize\n",
    "import spacy\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Activation, Dropout, LSTM, AveragePooling1D, Bidirectional, TimeDistributed, Flatten, Permute, Reshape, multiply, Lambda, RepeatVector, LeakyReLU, Concatenate, Masking\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "np.random.seed(1234)\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "ud1p0wVj0nZO",
    "outputId": "c5cc2446-64db-4f49-c4c5-11187c43d21d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/martontorner/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# download resources for spacy and nltk\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1272
    },
    "colab_type": "code",
    "id": "4D-DusSH6CbK",
    "outputId": "a90cde72-6dab-44d3-e8eb-e4f5ddebed4e"
   },
   "outputs": [],
   "source": [
    "# load pretrained model for demo (chunk_size=3 !!!)\n",
    "authors = np.load(\"authors_chunk3.serialized.npy\")\n",
    "model = load_model(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lv7YIqwy1avN"
   },
   "outputs": [],
   "source": [
    "def read_sentences_from_file(filepath):\n",
    "  data = \"\"\n",
    "  sentences = []\n",
    "\n",
    "  # parse file\n",
    "  with open(filepath, 'r') as file:\n",
    "      data=file.read()\n",
    "      \n",
    "  # split article into sentences\n",
    "  for sentence in tokenize.sent_tokenize(data):\n",
    "    sentences.append(sentence)\n",
    "  \n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfeRVkoZ2fRZ"
   },
   "outputs": [],
   "source": [
    "def parse_article(chunk_size, filepath):\n",
    "  n_words_per_chunk = chunk_size*25\n",
    "  word_repr_vector_size = 301\n",
    "  \n",
    "  sentence_shape = (n_words_per_chunk , word_repr_vector_size)\n",
    "  chunk_shape = (n_words_per_chunk , word_repr_vector_size)\n",
    "  \n",
    "  raw_sentences = read_sentences_from_file(filepath)\n",
    "  \n",
    "  n_sentences = len(raw_sentences)\n",
    "  \n",
    "  dataset = np.zeros([n_sentences-(chunk_size-1)], dtype=[('input', np.float32, sentence_shape)])\n",
    "  \n",
    "  # index dataset\n",
    "  index = 0\n",
    "  \n",
    "  article_parsed = np.empty([len(raw_sentences)], dtype=object)\n",
    "  \n",
    "  # parse sentences in article\n",
    "  for i, sentence in enumerate(raw_sentences):\n",
    "    sentence = raw_sentences[i]\n",
    "\n",
    "    parsed = np.empty((0, 301), np.float32)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "      # filter out stop words (not relevant/useful)\n",
    "      # 96 = punctuation char (->SpaCy documentation)\n",
    "      # if a word does not have vector form filter it out (very, very rare case)\n",
    "      if not token.is_stop and not token.pos == 96 and token.has_vector:\n",
    "        parsed = np.append(parsed, np.array([np.append(token.vector, float(token.pos)/100)]), axis=0)\n",
    "\n",
    "    article_parsed[i] = parsed\n",
    "\n",
    "  # for \"every sentence\" (the result is shorter, because the first and last sentences doesnt have enough neighbours)\n",
    "  for k in range(0, len(article_parsed)-chunk_size+1):\n",
    "    cursor = 0\n",
    "\n",
    "    # itarate for chunk_size from the actual sentence\n",
    "    for l in range(k, k+chunk_size):\n",
    "      for m in range(0, len(article_parsed[l])):\n",
    "        dataset[index][\"input\"][cursor] = article_parsed[l][m]\n",
    "        cursor += 1\n",
    "        if(cursor == n_words_per_chunk): break;\n",
    "      if(cursor == n_words_per_chunk): break;\n",
    "\n",
    "    index += 1\n",
    "       \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_7nIlFpACon"
   },
   "outputs": [],
   "source": [
    "def identify(filepath, model):\n",
    "  dataset = parse_article(chunk_size=3, filepath=filepath)\n",
    "  \n",
    "  predictions = model.predict(dataset[\"input\"])\n",
    "\n",
    "  sum = np.empty([len(predictions[0])],dtype=np.float32)\n",
    "  for pred in predictions:\n",
    "    sum += pred\n",
    "  \n",
    "  print(\"The identified author: \", authors[np.argmax(sum)] + \"\\n\\n\")\n",
    "\n",
    "  print(\"Predictions for each sentence bundle:\\n\")\n",
    "  print(\"%-25s %-15s %s\" % (\"Prediction\", \"Percent\", \"Real author\"))\n",
    "  print(\"=\"*55)\n",
    "  for i in range(0, len(predictions)):\n",
    "    print(\"%-25s %-15f %s\" % (authors[np.argmax(predictions[i])], predictions[i][np.argmax(predictions[i])]*100, \"AaronPressman\"))\n",
    "    print(\"-\"*55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0yhZkBB4aRd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The identified author:  AaronPressman\n",
      "\n",
      "\n",
      "Predictions for each sentence bundle:\n",
      "\n",
      "Prediction                Percent         Real author\n",
      "=======================================================\n",
      "RobinSidel                84.526813       AaronPressman\n",
      "-------------------------------------------------------\n",
      "PatriciaCommins           96.500832       AaronPressman\n",
      "-------------------------------------------------------\n",
      "AaronPressman             94.752622       AaronPressman\n",
      "-------------------------------------------------------\n",
      "AaronPressman             93.996990       AaronPressman\n",
      "-------------------------------------------------------\n",
      "AaronPressman             99.968553       AaronPressman\n",
      "-------------------------------------------------------\n",
      "RobinSidel                65.138346       AaronPressman\n",
      "-------------------------------------------------------\n",
      "SimonCowell               78.033686       AaronPressman\n",
      "-------------------------------------------------------\n",
      "SimonCowell               75.882185       AaronPressman\n",
      "-------------------------------------------------------\n",
      "ToddNissen                72.350270       AaronPressman\n",
      "-------------------------------------------------------\n",
      "ToddNissen                61.962354       AaronPressman\n",
      "-------------------------------------------------------\n",
      "PatriciaCommins           49.508867       AaronPressman\n",
      "-------------------------------------------------------\n",
      "ToddNissen                43.081662       AaronPressman\n",
      "-------------------------------------------------------\n",
      "NickLouth                 64.854133       AaronPressman\n",
      "-------------------------------------------------------\n",
      "AaronPressman             90.895289       AaronPressman\n",
      "-------------------------------------------------------\n",
      "SarahDavison              99.887460       AaronPressman\n",
      "-------------------------------------------------------\n",
      "SarahDavison              99.980253       AaronPressman\n",
      "-------------------------------------------------------\n",
      "SarahDavison              99.996603       AaronPressman\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "identify(\"article.txt\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "dl_project_demo.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
